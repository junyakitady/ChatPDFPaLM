{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ogaDN6kxNR"
      },
      "source": [
        "# PDF 文書の内容にもとづいて回答するボットの作り方を理解する\n",
        "## はじめに\n",
        "生成 AI や大規模言語モデル （LLM）は、膨大な量のテキストデータのトレーニングを受けており、テキストを生成し、言語を翻訳し、さまざまな種類のクリエイティブ コンテンツを作成し、有益な方法で質問に答えることができます。ただし、LLM には、企業で使用する場合の問題点がいくつかあります。\n",
        "その一つに、生成 AI は誤った情報を含むテキストを生成したり、誤った情報を翻訳したりする可能性があります。企業における生成 AI 利用では、これが問題となることがあります。\n",
        "\n",
        "そこで生成 AI をエンジンとして利用しながら、企業内の信頼のおける情報の中から適切な回答してほしいというユースケースがあります。\n",
        "\n",
        "これを実現する大きな処理の流れは次のようになっています。\n",
        "\n",
        "1.   PDF ファイルを読み込む\n",
        "2.   ページごとでもよいのですが、もう少し細かい単位にテキストを分割する\n",
        "3.   分割したテキスト情報を PaLM API を利用してベクトル化 / エンべディング\n",
        "4.   ベクトル情報とテキスト情報の関連を保持する\n",
        "5.   ユーザーの問い合わせ内容をベクトル化 / エンべディングする\n",
        "6.   5. のベクトルと最も類似度の高いベクトルを複数個取得する\n",
        "7.   6. のベクトルを生成元のテキスト情報を取得する\n",
        "8.   複数個のテキスト情報をコンテキスト情報として与え、この情報の中から質問の回答を作成するように LLM に問い合わせをする\n",
        "9.   PDF 文書の内容にもとづく回答が得られます\n",
        "\n",
        "これを実現するためにエンべディング、ベクトル検索やコンテンツの保持といった技術要素が必要となりますが、Google Cloud では Generetive AI App Builder - Enterprise Search というソリューションを提供しているため、個々の技術要素をバラバラに組み合わせるのではなく、Google の強みである検索技術をとりこんだソリューションを活用して簡単にソリューションを構築することが可能です。\n",
        "\n",
        "ここでは、生成 AI 利用時のフレームワークである LangChain を利用して、Enterprise Search を利用して企業の信頼するデータソースから情報を取得し、その情報をもとにChatbot型で質問に回答する流れを確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enterprise Search のセットアップ\n",
        "\n",
        "はじめに、Cloud Storage にバケットを作成し、回答させたいコンテンツを含む PDF ファイルをアップロードしておきます。\n",
        "このハンズオンでは、次のファイルをアップロードする前提で記載しています。\n",
        " - [Alphabet 2022 10K annual report](https://abc.xyz/assets/9a/bd/838c917c4b4ab21f94e84c3c2c65/goog-10-k-q4-2022.pdf)\n",
        " - [Google security overview](https://cloud.google.com/static/docs/security/overview/resources/google_security_wp.pdf)\n",
        " - [Google infrastructure security design overview](https://cloud.google.com/static/docs/security/infrastructure/design/resources/google_infrastructure_whitepaper_fa.pdf)\n",
        "\n",
        "次に、リンク先の手順に従って Enterprise Search をセットアップします。\n",
        "\n",
        "[Create and preview an unstructured data search ap](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search#create_and_preview_an_unstructured_data_search_app)\n",
        "\n",
        "パラメーターは以下を指定していることを確認してください。\n",
        "- アプリケーションの種類: 検索\n",
        "- インデックスするデータの種類: 非構造化データ( PDF や HTML )\n",
        "\n",
        "インデックスの作成にしばらく時間がかかります。Gen App Builder のプレビュー機能で検索ができるか確認することが可能です。\n",
        "\n",
        "最後に、Gen App Builder のアプリ一覧画面において、作成したアプリケーションの ID をメモしておきます。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YZN7V8SNq4-s"
      },
      "source": [
        "## 環境セットアップ\n",
        "\n",
        "python のバージョンを確認します。最新の LangChain は `Requires-Python >=3.8.1,<4.0`が前提となっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svXE7ZWa_sZj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "opZUiJat_s7V"
      },
      "source": [
        "前提パッケージを導入します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkYAr6d4jNuA"
      },
      "outputs": [],
      "source": [
        "# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform google-cloud-discoveryengine langchain==0.0.236  --upgrade --user"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJPoHHSPlMQw"
      },
      "source": [
        "※ 注意1\n",
        "\n",
        "* Colab の場合、上記のログに\"RESTART RUNTIME\"ボタンをが表示された場合、ボタンを押してカーネルをリスタートします。\n",
        "* Vertex AI Workbench の場合、メニューよりカーネルのリスタートを実行します。\n",
        "\n",
        "\n",
        "続いて、Google Cloud でプロジェクトを作成し Vertex AI API を有効化します。また、このコードを実行するユーザーに`Vertex AI ユーザー`のロールを付与します。\n",
        "\n",
        "\n",
        "※ 注意2\n",
        "* Colab の場合、以下をコメントアウトして Vertex AI API のユーザー権限をもつアカウントでログインします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-IsvQA9mP62"
      },
      "outputs": [],
      "source": [
        "#from google.colab import auth\n",
        "#auth.authenticate_user()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ytcQ_o2fC3Sm"
      },
      "source": [
        "環境変数などを定義します。 Google Cloud のプロジェクト ID、Enterprise Search の ID を指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou_H3H5FmyLb"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"<your_project_id>\"  # @param {type:\"string\"}\n",
        "SEARCH_ENGINE_ID = \"<search_engine_id>\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dym2cLCHDPh_"
      },
      "source": [
        "Vertex AI と LangChain のライブラリーの導入を確認します。 LangChain v0.0.208 で動作確認しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CyuusrQoFVw"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "198MC9RDrcTP"
      },
      "source": [
        "## Vertex AI PaLM API の準備\n",
        "\n",
        "LangChain を利用して PaLM API のText、Chat、Embeddings モデルを取得します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XO9IEnyo25g"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import VertexAI\n",
        "\n",
        "# Text model instance integrated with LangChain\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6LE-Bsw1OKO"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import GoogleCloudEnterpriseSearchRetriever\n",
        "\n",
        "# Enterprise Search retriever\n",
        "retriever = GoogleCloudEnterpriseSearchRetriever(project_id=PROJECT_ID, search_engine_id=SEARCH_ENGINE_ID, get_extractive_answers=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h8WbmIRuaxia"
      },
      "source": [
        "## Enterprise Search に格納した PDF の内容で Q/A をする\n",
        "\n",
        "ユーザーが質問した質問に対して、Enterprise Search の検索で取得した類似のテキスト情報の中から回答を出すようにします。この仕組みを簡単に実現するフレームワークとして LangChain の RetrievalQA を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lqTUnaD1SUw"
      },
      "outputs": [],
      "source": [
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R-lNIJn5bh0r"
      },
      "source": [
        "ここで質問を定義してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJC10UuF1m0W"
      },
      "outputs": [],
      "source": [
        "#query = input(\"Enter query:\")\n",
        "query = \"How does Google Cloud protect customer data?\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jE_8aSb_bseS"
      },
      "source": [
        "PaLM API for text に直接問い合わせをした場合はどのような回答になるでしょう？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTgFcvD8brgO"
      },
      "outputs": [],
      "source": [
        "llm(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Niy8GIieG7u"
      },
      "source": [
        "PDF 情報からの回答はどのようになるでしょう？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUriTu6Zbkqd"
      },
      "outputs": [],
      "source": [
        "response = qa({\"query\": query})\n",
        "response[\"result\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WLvzVhKD5LYV"
      },
      "source": [
        "回答内容を作成したソースを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTiKPdEikE1F"
      },
      "outputs": [],
      "source": [
        "response[\"source_documents\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc1KGL8510vo"
      },
      "source": [
        "どうして PDF の内容をもとに回答しているかを理解するには、上で利用した LangChain の RetrievalQA の `stuff` タイプのソースコードを参照するのが分かりやすいです。\n",
        "\n",
        "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/question_answering/stuff_prompt.py\n",
        "\n",
        "```\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "```\n",
        "訳\n",
        "```\n",
        "次のコンテキストをもとに、最後の質問に答えてください。もし答えが分からない場合は、分からないと答えてください。勝手に答えをでっち上げないでください。\n",
        "```\n",
        "\n",
        "以上です。ありがとうございました。\n",
        "\n",
        "## 参考情報\n",
        "- [Question Answering Over Documents](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gen-app-builder/retrieval-augmented-generation/examples/question_answering.ipynb)\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html)\n",
        "- [PaLM API on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
